{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "232ea983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'./en-ko.tmx' 파싱 시작...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TMX 파싱 중: 100%|██████████| 389633/389633 [00:00<00:00, 677400.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파싱 완료. 총 389632개의 문장 쌍 로드.\n",
      "데이터 셔플 중...\n",
      "로드된 데이터에서 15000개를 랜덤하게 선택합니다.\n",
      "선택된 데이터 15000개를 './prepared2_data.json' 파일로 저장 중...\n",
      "데이터 저장 완료.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import random\n",
    "import json\n",
    "from tqdm.auto import tqdm # 진행률 표시\n",
    "\n",
    "# --- 설정 ---\n",
    "TMX_FILE_PATH = './en-ko.tmx'  # !! 실제 TMX 파일 경로로 수정하세요 !!\n",
    "SOURCE_LANG_CODE = 'en'\n",
    "TARGET_LANG_CODE = 'ko'\n",
    "TOTAL_DATA_SIZE_TO_SELECT = 15000 # 선택할 총 데이터 개수\n",
    "RANDOM_SEED = 42\n",
    "OUTPUT_JSON_PATH = './prepared2_data.json' # 저장될 JSON 파일 경로\n",
    "\n",
    "# --- TMX 파싱 함수 (이전 코드와 동일) ---\n",
    "def parse_tmx(tmx_file, src_lang, tgt_lang):\n",
    "    pairs = []\n",
    "    print(f\"'{tmx_file}' 파싱 시작...\")\n",
    "    try:\n",
    "        tree = ET.parse(tmx_file)\n",
    "        root = tree.getroot()\n",
    "        # tqdm으로 진행률 표시 추가\n",
    "        for tu in tqdm(root.findall('.//tu'), desc=\"TMX 파싱 중\"):\n",
    "            source_seg = None\n",
    "            target_seg = None\n",
    "            for tuv in tu.findall('tuv'):\n",
    "                lang_attr = '{http://www.w3.org/XML/1998/namespace}lang'\n",
    "                lang = tuv.get(lang_attr) if tuv.get(lang_attr) else tuv.get('lang')\n",
    "                seg_element = tuv.find('seg')\n",
    "                if seg_element is not None and seg_element.text is not None:\n",
    "                    segment_text = seg_element.text.strip()\n",
    "                    if lang == src_lang:\n",
    "                        source_seg = segment_text\n",
    "                    elif lang == tgt_lang:\n",
    "                        target_seg = segment_text\n",
    "            if source_seg and target_seg:\n",
    "                # 간단한 필터링 예시 (빈 문자열 제외)\n",
    "                if source_seg and target_seg:\n",
    "                    pairs.append({'source': source_seg, 'target': target_seg})\n",
    "    except ET.ParseError as e:\n",
    "        print(f\"XML 파싱 오류: {e}\")\n",
    "        return None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"오류: 파일 '{tmx_file}'을(를) 찾을 수 없습니다.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"TMX 파싱 중 예상치 못한 오류 발생: {e}\")\n",
    "        return None\n",
    "    print(f\"파싱 완료. 총 {len(pairs)}개의 문장 쌍 로드.\")\n",
    "    return pairs\n",
    "\n",
    "# --- 메인 로직 ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 랜덤 시드 고정\n",
    "    random.seed(RANDOM_SEED)\n",
    "\n",
    "    # TMX 파일 파싱\n",
    "    all_sentence_pairs = parse_tmx(TMX_FILE_PATH, SOURCE_LANG_CODE, TARGET_LANG_CODE)\n",
    "\n",
    "    if all_sentence_pairs is None or not all_sentence_pairs:\n",
    "        print(\"데이터를 로드하지 못했습니다. 스크립트를 종료합니다.\")\n",
    "        exit()\n",
    "\n",
    "    # 데이터 셔플\n",
    "    print(\"데이터 셔플 중...\")\n",
    "    random.shuffle(all_sentence_pairs)\n",
    "\n",
    "    # 필요한 총 데이터 개수만큼 선택\n",
    "    selected_pairs = []\n",
    "    if len(all_sentence_pairs) < TOTAL_DATA_SIZE_TO_SELECT:\n",
    "        print(f\"경고: 로드된 데이터({len(all_sentence_pairs)}개)가 요청된 총 데이터 크기({TOTAL_DATA_SIZE_TO_SELECT}개)보다 적습니다.\")\n",
    "        print(f\"사용 가능한 모든 데이터를 사용합니다.\")\n",
    "        selected_pairs = all_sentence_pairs\n",
    "    else:\n",
    "        print(f\"로드된 데이터에서 {TOTAL_DATA_SIZE_TO_SELECT}개를 랜덤하게 선택합니다.\")\n",
    "        selected_pairs = all_sentence_pairs[:TOTAL_DATA_SIZE_TO_SELECT]\n",
    "\n",
    "    # 선택된 데이터를 JSON 파일로 저장\n",
    "    print(f\"선택된 데이터 {len(selected_pairs)}개를 '{OUTPUT_JSON_PATH}' 파일로 저장 중...\")\n",
    "    try:\n",
    "        with open(OUTPUT_JSON_PATH, 'w', encoding='utf-8') as f:\n",
    "            json.dump(selected_pairs, f, ensure_ascii=False, indent=4)\n",
    "        print(\"데이터 저장 완료.\")\n",
    "    except IOError as e:\n",
    "        print(f\"파일 저장 중 오류 발생: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"데이터 저장 중 예상치 못한 오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72dd7759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'./prepared2_data.json'에서 데이터 로드 중...\n",
      "총 15000개의 문장 쌍 로드 완료.\n",
      "데이터 분할 완료: 훈련 10000개, 검증 2500개, 테스트 2500개\n",
      "Hugging Face DatasetDict 생성 완료.\n",
      "\n",
      "'google/mt5-xl' 토크나이저 로딩 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KDP-34\\anaconda3\\envs\\new_NLP\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토크나이저 로딩 성공.\n",
      "\n",
      "데이터셋 토큰화 중...\n",
      "사용 가능한 CPU 코어 수: 8. 토큰화에 활용합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):   0%|          | 0/10000 [00:02<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRemoteTraceback\u001b[39m                           Traceback (most recent call last)",
      "\u001b[31mRemoteTraceback\u001b[39m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\KDP-34\\anaconda3\\envs\\new_NLP\\Lib\\site-packages\\multiprocess\\pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\KDP-34\\anaconda3\\envs\\new_NLP\\Lib\\site-packages\\datasets\\utils\\py_utils.py\", line 680, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n  File \"c:\\Users\\KDP-34\\anaconda3\\envs\\new_NLP\\Lib\\site-packages\\datasets\\arrow_dataset.py\", line 3516, in _map_single\n    for i, batch in iter_outputs(shard_iterable):\n  File \"c:\\Users\\KDP-34\\anaconda3\\envs\\new_NLP\\Lib\\site-packages\\datasets\\arrow_dataset.py\", line 3466, in iter_outputs\n    yield i, apply_function(example, i, offset=offset)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\KDP-34\\anaconda3\\envs\\new_NLP\\Lib\\site-packages\\datasets\\arrow_dataset.py\", line 3389, in apply_function\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\KDP-34\\AppData\\Local\\Temp\\ipykernel_11420\\3003112208.py\", line 99, in preprocess_function\nNameError: name 'AutoTokenizer' is not defined\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 110\u001b[39m\n\u001b[32m    107\u001b[39m num_cores = os.cpu_count()\n\u001b[32m    108\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m사용 가능한 CPU 코어 수: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_cores\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. 토큰화에 활용합니다.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m tokenized_datasets = \u001b[43mfinal_datasets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreprocess_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_cores\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfinal_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel_ckpt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_CHECKPOINT\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# fn_kwargs를 통해 MODEL_CHECKPOINT 전달\u001b[39;49;00m\n\u001b[32m    116\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m데이터셋 토큰화 완료.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# --- 5. 모델 로드 ---\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KDP-34\\anaconda3\\envs\\new_NLP\\Lib\\site-packages\\datasets\\dataset_dict.py:941\u001b[39m, in \u001b[36mDatasetDict.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, with_split, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[32m    939\u001b[39m     function = bind(function, split)\n\u001b[32m--> \u001b[39m\u001b[32m941\u001b[39m dataset_dict[split] = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[32m    962\u001b[39m     function = function.func\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KDP-34\\anaconda3\\envs\\new_NLP\\Lib\\site-packages\\datasets\\arrow_dataset.py:557\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    550\u001b[39m self_format = {\n\u001b[32m    551\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    552\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    553\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    554\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    555\u001b[39m }\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    558\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KDP-34\\anaconda3\\envs\\new_NLP\\Lib\\site-packages\\datasets\\arrow_dataset.py:3166\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[39m\n\u001b[32m   3160\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSpawning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m processes\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   3161\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[32m   3162\u001b[39m     unit=\u001b[33m\"\u001b[39m\u001b[33m examples\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3163\u001b[39m     total=pbar_total,\n\u001b[32m   3164\u001b[39m     desc=(desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mMap\u001b[39m\u001b[33m\"\u001b[39m) + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m (num_proc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3165\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m3166\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miflatmap_unordered\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3167\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs_iterable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs_per_job\u001b[49m\n\u001b[32m   3168\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3169\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3170\u001b[39m \u001b[43m            \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KDP-34\\anaconda3\\envs\\new_NLP\\Lib\\site-packages\\datasets\\utils\\py_utils.py:720\u001b[39m, in \u001b[36miflatmap_unordered\u001b[39m\u001b[34m(pool, func, kwargs_iterable)\u001b[39m\n\u001b[32m    717\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    718\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[32m    719\u001b[39m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         \u001b[43m[\u001b[49m\u001b[43masync_result\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43masync_result\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43masync_results\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KDP-34\\anaconda3\\envs\\new_NLP\\Lib\\site-packages\\datasets\\utils\\py_utils.py:720\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    717\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    718\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[32m    719\u001b[39m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         [\u001b[43masync_result\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KDP-34\\anaconda3\\envs\\new_NLP\\Lib\\site-packages\\multiprocess\\pool.py:774\u001b[39m, in \u001b[36mApplyResult.get\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    772\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n",
      "\u001b[31mNameError\u001b[39m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- 0. 설정 (Configuration) ---\n",
    "INPUT_JSON_PATH = './prepared2_data.json'\n",
    "MODEL_CHECKPOINT = \"google/mt5-xl\"\n",
    "TRAIN_SIZE = 10000\n",
    "VAL_SIZE = 2500\n",
    "TEST_SIZE = 2500\n",
    "RANDOM_SEED = 42\n",
    "OUTPUT_DIR = f\"./{MODEL_CHECKPOINT.replace('/', '_')}-plz_file\"\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "EVAL_BATCH_SIZE = 4\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 1\n",
    "WEIGHT_DECAY = 0.01\n",
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "SAVE_STEPS = 500\n",
    "EVAL_STEPS = 500\n",
    "LOGGING_STEPS = 100\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# --- 1. 준비된 데이터 로드 및 분할 ---\n",
    "print(f\"'{INPUT_JSON_PATH}'에서 데이터 로드 중...\")\n",
    "try:\n",
    "    with open(INPUT_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "        loaded_pairs = json.load(f)\n",
    "    print(f\"총 {len(loaded_pairs)}개의 문장 쌍 로드 완료.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: 파일 '{INPUT_JSON_PATH}'을(를) 찾을 수 없습니다. 먼저 데이터 준비 스크립트를 실행하세요.\")\n",
    "    exit()\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"오류: '{INPUT_JSON_PATH}' 파일이 올바른 JSON 형식이 아닙니다.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"데이터 로드 중 예상치 못한 오류 발생: {e}\")\n",
    "    exit()\n",
    "\n",
    "total_loaded = len(loaded_pairs)\n",
    "expected_total = TRAIN_SIZE + VAL_SIZE + TEST_SIZE\n",
    "if total_loaded < expected_total:\n",
    "    print(f\"경고: 로드된 데이터({total_loaded}개)가 필요한 총 데이터({expected_total}개)보다 적습니다.\")\n",
    "    print(\"사용 가능한 데이터에 맞춰 분할 크기를 조정합니다.\")\n",
    "    ratio = total_loaded / expected_total\n",
    "    TRAIN_SIZE = int(TRAIN_SIZE * ratio)\n",
    "    VAL_SIZE = int(VAL_SIZE * ratio)\n",
    "    TEST_SIZE = total_loaded - TRAIN_SIZE - VAL_SIZE\n",
    "    print(f\"조정된 크기: 훈련={TRAIN_SIZE}, 검증={VAL_SIZE}, 테스트={TEST_SIZE}\")\n",
    "elif total_loaded > expected_total:\n",
    "    print(f\"경고: 로드된 데이터({total_loaded}개)가 필요한 총 데이터({expected_total}개)보다 많습니다.\")\n",
    "    print(f\"{expected_total}개만 사용합니다.\")\n",
    "    loaded_pairs = loaded_pairs[:expected_total]\n",
    "\n",
    "train_pairs_dict = loaded_pairs[:TRAIN_SIZE]\n",
    "val_pairs_dict = loaded_pairs[TRAIN_SIZE : TRAIN_SIZE + VAL_SIZE]\n",
    "test_pairs_dict = loaded_pairs[TRAIN_SIZE + VAL_SIZE :]\n",
    "\n",
    "print(f\"데이터 분할 완료: 훈련 {len(train_pairs_dict)}개, 검증 {len(val_pairs_dict)}개, 테스트 {len(test_pairs_dict)}개\")\n",
    "\n",
    "final_datasets = DatasetDict({\n",
    "    'train': Dataset.from_list(train_pairs_dict),\n",
    "    'validation': Dataset.from_list(val_pairs_dict),\n",
    "    'test': Dataset.from_list(test_pairs_dict)\n",
    "})\n",
    "print(\"Hugging Face DatasetDict 생성 완료.\")\n",
    "\n",
    "# --- 2. 토크나이저 로드 및 최대 길이 계산 ---\n",
    "print(f\"\\n'{MODEL_CHECKPOINT}' 토크나이저 로딩 중...\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "    print(\"토크나이저 로딩 성공.\")\n",
    "except Exception as e:\n",
    "    print(f\"토크나이저 로딩 오류: {e}\")\n",
    "    exit()\n",
    "\n",
    "max_source_length = 8\n",
    "max_target_length = 8\n",
    "\n",
    "# --- 3. 데이터셋 토큰화 ---\n",
    "def preprocess_function(examples, model_ckpt): # model_ckpt 인자 추가\n",
    "    inputs = examples['source']\n",
    "    targets = examples['target']\n",
    "    tokenizer_local = AutoTokenizer.from_pretrained(model_ckpt) # model_ckpt 사용\n",
    "    model_inputs = tokenizer_local(inputs, max_length=max_source_length, truncation=True)\n",
    "    with tokenizer_local.as_target_tokenizer():\n",
    "        labels = tokenizer_local(targets, max_length=max_target_length, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "print(\"\\n데이터셋 토큰화 중...\")\n",
    "num_cores = os.cpu_count()\n",
    "print(f\"사용 가능한 CPU 코어 수: {num_cores}. 토큰화에 활용합니다.\")\n",
    "\n",
    "tokenized_datasets = final_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=max(1, num_cores // 2),\n",
    "    remove_columns=final_datasets[\"train\"].column_names,\n",
    "    fn_kwargs={'model_ckpt': MODEL_CHECKPOINT} # fn_kwargs를 통해 MODEL_CHECKPOINT 전달\n",
    ")\n",
    "print(\"데이터셋 토큰화 완료.\")\n",
    "\n",
    "# --- 5. 모델 로드 ---\n",
    "print(f\"\\n'{MODEL_CHECKPOINT}' 사전 훈련된 모델 로딩 중...\")\n",
    "try:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT)\n",
    "    print(\"모델 로딩 성공.\")\n",
    "except Exception as e:\n",
    "    print(f\"모델 로딩 오류: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 6. 데이터 콜레이터 정의 ---\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "print(\"\\n데이터 콜레이터 준비 완료.\")\n",
    "\n",
    "# --- 7. 평가 지표 정의 (BLEU) ---\n",
    "try:\n",
    "    bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "    print(\"\\n평가 지표 (sacrebleu) 로딩 성공.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nsacrebleu 지표 로딩 실패: {e}. BLEU 점수 계산이 비활성화될 수 있습니다.\")\n",
    "    bleu_metric = None\n",
    "\n",
    "# compute_metrics 함수는 tokenizer를 전역 범위에서 참조하므로 수정 필요 없음\n",
    "def compute_metrics(eval_preds):\n",
    "    if bleu_metric is None: return {}\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple): preds = preds[0]\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    decoded_labels_for_bleu = [[label.strip()] for label in decoded_labels]\n",
    "    decoded_preds_for_bleu = [pred.strip() for pred in decoded_preds]\n",
    "    result = bleu_metric.compute(predictions=decoded_preds_for_bleu, references=decoded_labels_for_bleu)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "    prediction_lens = np.sum(preds != tokenizer.pad_token_id, axis=1)\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result\n",
    "\n",
    "# --- 8. 훈련 인자(Arguments) 정의 ---\n",
    "print(\"\\n훈련 인자 설정 중 (CPU 사용)...\")\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    eval_strategy=\"steps\",\n",
    "    fp16=False,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    report_to=\"tensorboard\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"bleu\",\n",
    "    greater_is_better=True,\n",
    "    no_cuda=True,\n",
    ")\n",
    "\n",
    "# --- 9. Trainer 생성 ---\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics if bleu_metric else None,\n",
    "    tokenizer=tokenizer, # Trainer에는 tokenizer를 전달해야 함\n",
    ")\n",
    "print(\"Trainer 준비 완료.\")\n",
    "\n",
    "# --- 10. 미세 조정 시작 ---\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"미세 조정을 시작합니다 (CPU 사용).\")\n",
    "print(f\"총 {NUM_EPOCHS} 에폭 동안 훈련합니다. 시간이 매우 오래 걸릴 수 있습니다.\")\n",
    "print(\"=\"*30 + \"\\n\")\n",
    "\n",
    "try:\n",
    "    print(\"훈련 시작...\")\n",
    "    train_result = trainer.train(resume_from_checkpoint=None)\n",
    "    print(\"훈련 완료.\")\n",
    "\n",
    "    # --- 11. 최종 모델 저장 ---\n",
    "    print(f\"\\n훈련 중 최고 성능 모델이 '{training_args.output_dir}' 내 checkpoint 폴더에 저장되었습니다.\")\n",
    "    print(f\"훈련 종료 후 최고 성능 모델이 로드되었습니다.\")\n",
    "\n",
    "    metrics = train_result.metrics\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "\n",
    "    # --- 12. 훈련 후 최종 평가 ---\n",
    "    print(\"\\n최고 성능 모델 평가 중 (검증 데이터셋)...\")\n",
    "    eval_metrics = trainer.evaluate(eval_dataset=tokenized_datasets[\"validation\"])\n",
    "    trainer.log_metrics(\"eval_validation\", eval_metrics)\n",
    "    trainer.save_metrics(\"eval_validation\", eval_metrics)\n",
    "    print(f\"최고 성능 모델 검증 데이터셋 평가 결과: {eval_metrics}\")\n",
    "\n",
    "    print(\"\\n최고 성능 모델 평가 중 (테스트 데이터셋)...\")\n",
    "    test_metrics = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])\n",
    "    trainer.log_metrics(\"eval_test\", test_metrics)\n",
    "    trainer.save_metrics(\"eval_test\", test_metrics)\n",
    "    print(f\"최고 성능 모델 테스트 데이터셋 평가 결과: {test_metrics}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n훈련 또는 평가 중 오류 발생: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n스크립트 실행 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dfbf87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
